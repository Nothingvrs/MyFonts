"""
–°–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏–∑–∞ —à—Ä–∏—Ñ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PaddleOCR –∏ OpenCV
"""

import cv2
import numpy as np
from PIL import Image
import io
import logging
from typing import Tuple, List, Dict, Any
import asyncio
from concurrent.futures import ThreadPoolExecutor

from ..models.font_models import FontCharacteristics, CyrillicFeatures
from .paddleocr_service import PaddleOCRService

logger = logging.getLogger(__name__)


class FontAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —à—Ä–∏—Ñ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ PaddleOCR –∏ OpenCV"""
    
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.paddleocr_service = PaddleOCRService()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å PaddleOCR
        if self.paddleocr_service.is_available():
            logger.info("‚úÖ FontAnalyzer: PaddleOCR —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            logger.error("‚ùå FontAnalyzer: PaddleOCR –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω - –∞–Ω–∞–ª–∏–∑ —à—Ä–∏—Ñ—Ç–æ–≤ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω")
        
    async def analyze_image(self, image_bytes: bytes) -> FontCharacteristics:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —à—Ä–∏—Ñ—Ç–∞"""
        return await self._analyze_image_async(image_bytes)
    
    async def _analyze_image_async(self, image_bytes: bytes) -> FontCharacteristics:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¢–û–õ–¨–ö–û —á–µ—Ä–µ–∑ PaddleOCR"""
        print("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: _analyze_image_async –ù–ê–ß–ê–õ–°–Ø")
        logger.info("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: _analyze_image_async –ù–ê–ß–ê–õ–°–Ø")
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            print("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
            logger.info("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...")
            image = self._load_image(image_bytes)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PaddleOCR
            logger.info("=== –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–û–°–¢–ò PADDLEOCR ===")
            if not hasattr(self, 'paddleocr_service') or not self.paddleocr_service:
                logger.error("‚ùå PaddleOCR —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
                raise ValueError("–ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —à—Ä–∏—Ñ—Ç–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.")
            
            if not self.paddleocr_service.is_available():
                logger.error("‚ùå PaddleOCR –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
                raise ValueError("–ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —à—Ä–∏—Ñ—Ç–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.")
            
            logger.info("‚úÖ PaddleOCR –¥–æ—Å—Ç—É–ø–µ–Ω - –Ω–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑")
            
            # –®–ê–ì 1: –£–õ–£–ß–®–ï–ù–ù–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ PaddleOCR
            logger.info("=== –®–ê–ì 1: –£–õ–£–ß–®–ï–ù–ù–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ PaddleOCR ===")
            print("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –í—ã–∑—ã–≤–∞–µ–º PaddleOCR.analyze_image()")
            logger.info("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –í—ã–∑—ã–≤–∞–µ–º PaddleOCR.analyze_image()")
            ocr_result = await self.paddleocr_service.analyze_image(image)
            print(f"üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: PaddleOCR –≤–µ—Ä–Ω—É–ª: {type(ocr_result)}")
            logger.info(f"üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: PaddleOCR –≤–µ—Ä–Ω—É–ª: {type(ocr_result)}")
            
            # –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            logger.info(f"üîç PADDLEOCR –†–ï–ó–£–õ–¨–¢–ê–¢:")
            logger.info(f"  - has_text: {ocr_result.get('has_text', False)}")
            logger.info(f"  - text_content: '{ocr_result.get('text_content', '')[:50]}...'")
            logger.info(f"  - confidence: {ocr_result.get('confidence', 0.0):.3f}")
            logger.info(f"  - regions_count: {ocr_result.get('regions_count', 0)}")
            logger.info(f"  - error: {ocr_result.get('error', '–Ω–µ—Ç')}")
            
            # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ç–µ–∫—Å—Ç–∞
            text_validation = self._validate_text_presence(ocr_result)
            if not text_validation['is_valid']:
                logger.info(f"–†–ï–ó–£–õ–¨–¢–ê–¢ –®–ê–ì 1: –¢–µ–∫—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é - {text_validation['reason']}")
                raise ValueError(f"–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç: {text_validation['reason']}")
            
            logger.info("–†–ï–ó–£–õ–¨–¢–ê–¢ –®–ê–ì 1: ‚úÖ –¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
            
            # –®–ê–ì 2: –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤ —á–µ—Ä–µ–∑ PaddleOCR
            logger.info("=== –®–ê–ì 2: –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤ —á–µ—Ä–µ–∑ PaddleOCR ===")
            # –£–≤–∞–∂–∞–µ–º —Ñ–ª–∞–≥, —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ PaddleOCR
            if ocr_result.get('multiple_fonts', False):
                logger.info("–†–ï–ó–£–õ–¨–¢–ê–¢ –®–ê–ì 2: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à—Ä–∏—Ñ—Ç–æ–≤ (–∏–∑ PaddleOCR) - –°–¢–û–ü")
                raise ValueError("–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤. –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º –æ–¥–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞.")
            if await self._detect_multiple_fonts_from_ocr_result(ocr_result):
                logger.info("–†–ï–ó–£–õ–¨–¢–ê–¢ –®–ê–ì 2: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à—Ä–∏—Ñ—Ç–æ–≤ - –°–¢–û–ü")
                raise ValueError("–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤. –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º –æ–¥–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞.")
            logger.info("–†–ï–ó–£–õ–¨–¢–ê–¢ –®–ê–ì 2: ‚úÖ –û–¥–∏–Ω —à—Ä–∏—Ñ—Ç - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫ –∞–Ω–∞–ª–∏–∑—É")
            
            # –®–ê–ì 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —à—Ä–∏—Ñ—Ç–∞ —á–µ—Ä–µ–∑ PaddleOCR
            logger.info("=== –®–ê–ì 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —à—Ä–∏—Ñ—Ç–∞ —á–µ—Ä–µ–∑ PaddleOCR ===")
            characteristics = await self._extract_characteristics_from_ocr(image, ocr_result)
            logger.info("–†–ï–ó–£–õ–¨–¢–ê–¢ –®–ê–ì 3: ‚úÖ –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã")
            
            # –®–ê–ì 4: –°–≤–µ—Ä–∫–∞ —Å –±–∞–∑–æ–π —à—Ä–∏—Ñ—Ç–æ–≤ (–±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤ font_matcher)
            logger.info("=== –®–ê–ì 4: –°–≤–µ—Ä–∫–∞ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ —à—Ä–∏—Ñ—Ç–æ–≤ –≤ –±–∞–∑–µ ===")
            logger.info("–†–ï–ó–£–õ–¨–¢–ê–¢ –®–ê–ì 4: ‚úÖ –ì–æ—Ç–æ–≤–æ –∫ —Å–≤–µ—Ä–∫–µ —Å –±–∞–∑–æ–π (—Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ font_matcher)")
            
            # –®–ê–ì 5: –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
            logger.info("=== –®–ê–ì 5: –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ ===")
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ —á–µ—Ä–µ–∑ PaddleOCR")
            print("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            logger.info("üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            return characteristics
            
        except ValueError as logic_error:
            # –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ (–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞, –º–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–æ–≤) - –ø–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            print(f"üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –õ–æ–≥–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(logic_error)}")
            logger.info(f"‚ÑπÔ∏è –õ–æ–≥–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: {str(logic_error)}")
            raise logic_error
            
        except Exception as error:
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
            print(f"üöÄ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(error)}")
            logger.error(f"‚ùå –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(error)}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –∏ –¥–∞–µ–º –ø–æ–Ω—è—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            if "PaddleOCR –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω" in str(error):
                user_message = "–ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —à—Ä–∏—Ñ—Ç–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
            elif "PaddleOCR —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω" in str(error):
                user_message = "–°–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏–∑–∞ —à—Ä–∏—Ñ—Ç–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            elif "–Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ" in str(error):
                user_message = "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞."
            else:
                user_message = "–ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            
            raise ValueError(user_message)
    
    def _validate_text_presence(self, ocr_result: dict) -> dict:
        """–°–¢–†–û–ì–ê–Ø –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞–ª–∏—á–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        try:
            logger.info("=== –°–¢–†–û–ì–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø –ù–ê–õ–ò–ß–ò–Ø –¢–ï–ö–°–¢–ê ===")
            
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            has_text = ocr_result.get('has_text', False)
            text_content = ocr_result.get('text_content', '').strip()
            confidence = ocr_result.get('confidence', 0.0)
            regions_count = ocr_result.get('regions_count', 0)
            text_regions = ocr_result.get('text_regions', [])
            
            logger.info(f"üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
            logger.info(f"  - has_text: {has_text}")
            logger.info(f"  - text_content: '{text_content}'")
            logger.info(f"  - text_length: {len(text_content)}")
            logger.info(f"  - confidence: {confidence:.3f}")
            logger.info(f"  - regions_count: {regions_count}")
            logger.info(f"  - text_regions: {len(text_regions)}")
            
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–≥–æ —Ñ–ª–∞–≥–∞ OCR
            if not has_text:
                return {
                    'is_valid': False,
                    'reason': '–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç',
                    'details': 'PaddleOCR –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏'
                }
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            if not text_content or len(text_content.strip()) < 1:
                return {
                    'is_valid': False,
                    'reason': '–ù–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π',
                    'details': f'–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤'
                }
            
            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            if confidence < 0.05:  # –ï—â–µ –±–æ–ª—å—à–µ –ø–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥
                return {
                    'is_valid': False,
                    'reason': '–ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–æ–µ',
                    'details': f'–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {confidence:.2f} (–º–∏–Ω–∏–º—É–º: 0.05)'
                }
            
            # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
            if regions_count < 1 or len(text_regions) < 1:
                return {
                    'is_valid': False,
                    'reason': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ —Å —Ç–µ–∫—Å—Ç–æ–º',
                    'details': f'–†–µ–≥–∏–æ–Ω–æ–≤: {regions_count}, –æ–±–ª–∞—Å—Ç–µ–π: {len(text_regions)}'
                }
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞
            valid_regions = 0
            for region in text_regions:
                region_conf = region.get('confidence', 0)
                region_text = region.get('text', '').strip()
                
                if region_conf >= 0.05 and len(region_text) >= 1:  # –ï—â–µ –±–æ–ª—å—à–µ –ø–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥
                    valid_regions += 1
            
            if valid_regions < 1:
                return {
                    'is_valid': False,
                    'reason': '–ù–∏ –æ–¥–Ω–∞ –æ–±–ª–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –Ω–µ –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞',
                    'details': f'–í–∞–ª–∏–¥–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π: {valid_regions} –∏–∑ {len(text_regions)}'
                }
            
            # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
            # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Å—Ç–∞–ª–∏—Å—å –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã
            clean_text = ''.join(c for c in text_content if c.isalnum() or c.isspace()).strip()
            if len(clean_text) < 1:  # –ü–æ–Ω–∏–∂–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –¥–æ 1 —Å–∏–º–≤–æ–ª–∞
                return {
                    'is_valid': False,
                    'reason': '–¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Ç–∞–µ–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤',
                    'details': f'–ß–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç: "{clean_text}" (–¥–ª–∏–Ω–∞: {len(clean_text)})'
                }
            
            # 7. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±—É–∫–≤ –∏–ª–∏ —Ü–∏—Ñ—Ä
            letter_count = sum(1 for c in text_content if c.isalpha())
            digit_count = sum(1 for c in text_content if c.isdigit())
            if letter_count < 1 and digit_count < 1:  # –†–∞–∑—Ä–µ—à–∞–µ–º —Ü–∏—Ñ—Ä—ã
                return {
                    'is_valid': False,
                    'reason': '–¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –±—É–∫–≤ –∏–ª–∏ —Ü–∏—Ñ—Ä',
                    'details': f'–ë—É–∫–≤: {letter_count}, —Ü–∏—Ñ—Ä: {digit_count}'
                }
            
            # 8. –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º)
            cyrillic_chars = sum(1 for char in text_content if 1040 <= ord(char) <= 1103)
            if cyrillic_chars == 0:
                logger.warning("‚ö†Ô∏è –¢–µ–∫—Å—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–º–∏")
            
            # –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã
            logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return {
                'is_valid': True,
                'reason': '–¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à–µ–ª –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞',
                'details': f'–¢–µ–∫—Å—Ç: "{text_content[:50]}..." (–¥–ª–∏–Ω–∞: {len(text_content)}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}, —Ä–µ–≥–∏–æ–Ω–æ–≤: {valid_regions})'
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return {
                'is_valid': False,
                'reason': f'–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞',
                'details': f'–û—à–∏–±–∫–∞: {str(e)}'
            }
    
    def _assess_text_quality(self, text_content: str, confidence: float, regions_count: int) -> dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        try:
            score = 0.0
            reasons = []
            
            # 1. –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ OCR (–≤–µ—Å: 40%)
            confidence_score = min(1.0, confidence / 0.8)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0.8 –∫–∞–∫ –º–∞–∫—Å–∏–º—É–º
            score += confidence_score * 0.4
            
            # 2. –û—Ü–µ–Ω–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ä–µ–≥–∏–æ–Ω–æ–≤ (–≤–µ—Å: 25%)
            # –ë–æ–ª—å—à–µ —Ä–µ–≥–∏–æ–Ω–æ–≤ = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
            if regions_count >= 3 and regions_count <= 20:
                regions_score = 1.0
            elif regions_count > 20:
                regions_score = 0.7  # –ú–Ω–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–æ–≤ –º–æ–∂–µ—Ç –æ–∑–Ω–∞—á–∞—Ç—å —à—É–º
            else:
                regions_score = regions_count / 3.0
            
            score += regions_score * 0.25
            
            # 3. –û—Ü–µ–Ω–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Ç–µ–∫—Å—Ç–∞ (–≤–µ—Å: 35%)
            content_score = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            meaningful_chars = sum(1 for char in text_content if char.isalnum() or char.isspace())
            if len(text_content) > 0:
                meaningful_ratio = meaningful_chars / len(text_content)
                content_score += meaningful_ratio * 0.5
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
            words = text_content.split()
            if len(words) >= 2:
                word_lengths = [len(word) for word in words]
                avg_word_length = sum(word_lengths) / len(word_lengths)
                if 2 <= avg_word_length <= 8:  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤
                    content_score += 0.3
                elif avg_word_length > 8:
                    content_score += 0.1  # –î–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—à–∏–±–∫–∞–º–∏ OCR
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            cyrillic_chars = sum(1 for char in text_content if ord(char) >= 1040 and ord(char) <= 1103)
            if cyrillic_chars > 0:
                content_score += 0.2
            
            score += content_score * 0.35
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            is_good = score >= 0.6
            reason = "–•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ" if is_good else "–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
            
            if score < 0.4:
                reason = "–û—á–µ–Ω—å –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
            elif score < 0.6:
                reason = "–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
            elif score < 0.8:
                reason = "–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
            else:
                reason = "–í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
            
            return {
                'is_good': is_good,
                'score': score,
                'reason': reason,
                'details': {
                    'confidence_score': confidence_score,
                    'regions_score': regions_score,
                    'content_score': content_score
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return {
                'is_good': False,
                'score': 0.0,
                'reason': f'–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {str(e)}',
                'details': {}
            }
    
    def _load_image(self, image_bytes: bytes) -> np.ndarray:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –±–∞–π—Ç–æ–≤"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º PIL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            pil_image = Image.open(io.BytesIO(image_bytes))
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array (RGB –¥–ª—è PaddleOCR)
            cv_image = np.array(pil_image)  # –û—Å—Ç–∞–≤–ª—è–µ–º RGB —Ñ–æ—Ä–º–∞—Ç –¥–ª—è PaddleOCR
            
            logger.info(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {cv_image.shape}, —Ñ–æ—Ä–º–∞—Ç: RGB")
            return cv_image
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {str(e)}")
    

    

    

    

    

    

    

    

    
    async def _detect_multiple_fonts_from_ocr_result(self, ocr_result: dict) -> bool:
        """–¢–æ—á–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        try:
            logger.info("=== –¢–û–ß–ù–ê–Ø –î–ï–¢–ï–ö–¶–ò–Ø –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–• –®–†–ò–§–¢–û–í ===")
            
            if not ocr_result.get('has_text', False):
                logger.info("OCR –Ω–µ –Ω–∞—à–µ–ª —Ç–µ–∫—Å—Ç - –æ–¥–∏–Ω —à—Ä–∏—Ñ—Ç")
                return False
            
            text_content = ocr_result.get('text_content', '').strip()
            regions_count = ocr_result.get('regions_count', 0)
            text_regions = ocr_result.get('text_regions', [])
            confidence = ocr_result.get('confidence', 0.0)
            
            logger.info(f"–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö: '{text_content[:50]}...' ({regions_count} —Ä–µ–≥–∏–æ–Ω–æ–≤, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–ª–∞—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞: {len(text_regions)}")
            
            # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤
            if regions_count < 2 or len(text_regions) < 2:
                logger.info(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±–ª–∞—Å—Ç–µ–π ({len(text_regions)}) –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤")
                return False
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–æ–≤—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤: –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –∏ –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            filtered_regions = []
            for r in text_regions:
                txt = str(r.get('text', '')).strip()
                conf = float(r.get('confidence', 0.0))
                if len(txt) >= 2 and conf >= 0.6:
                    filtered_regions.append(r)

            if len(filtered_regions) < 2:
                logger.info("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —à—É–º–æ–≤ –æ—Å—Ç–∞–ª–æ—Å—å < 2 —Ä–µ–≥–∏–æ–Ω–æ–≤ ‚Äî —Å—á–∏—Ç–∞–µ–º –æ–¥–∏–Ω —à—Ä–∏—Ñ—Ç")
                return False

            # –†–∞–Ω–Ω–∏–π –∫—Ä–∏—Ç–µ—Ä–∏–π –æ–¥–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞: –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –∫–ª–∞—Å—Ç–µ—Ä –≤—ã—Å–æ—Ç
            heights = [r.get('height', 0) for r in filtered_regions if r.get('height', 0) > 5]
            if len(heights) >= 3:
                import numpy as np
                h_arr = np.array(heights, dtype=float)
                median_h = float(np.median(h_arr))
                if median_h > 0:
                    in_band = np.logical_and(h_arr >= 0.7 * median_h, h_arr <= 1.3 * median_h)
                    frac_in_band = float(np.sum(in_band)) / float(len(h_arr))
                    logger.info(f"–î–æ–ª—è –≤—ã—Å–æ—Ç –≤ [0.7..1.3] –æ—Ç –º–µ–¥–∏–∞–Ω—ã: {frac_in_band:.2f}")
                    if frac_in_band >= 0.8:
                        logger.info("‚úÖ –î–æ–º–∏–Ω–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä –≤—ã—Å–æ—Ç (>=80%) ‚Äî —Å—á–∏—Ç–∞–µ–º –æ–¥–∏–Ω —à—Ä–∏—Ñ—Ç")
                        return False

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö
            multiple_fonts_detected = await self._advanced_multiple_fonts_detection(filtered_regions, text_content)
            
            if multiple_fonts_detected:
                logger.info("‚úÖ –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –®–†–ò–§–¢–´")
            else:
                logger.info("‚úÖ –û–î–ò–ù –®–†–ò–§–¢")
            
            return multiple_fonts_detected
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤: {str(e)}")
            logger.warning("‚ö†Ô∏è –ü—Ä–∏ –æ—à–∏–±–∫–µ —Å—á–∏—Ç–∞–µ–º –æ–¥–∏–Ω —à—Ä–∏—Ñ—Ç")
            return False
    
    async def _advanced_multiple_fonts_detection(self, text_regions: list, text_content: str) -> bool:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤"""
        try:
            logger.info("=== –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –î–ï–¢–ï–ö–¶–ò–Ø –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–• –®–†–ò–§–¢–û–í ===")
            # 0) –ñ—ë—Å—Ç–∫–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —à—É–º: –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏, –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –Ω—É–ª–µ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã
            filtered = []
            for r in text_regions:
                txt = str(r.get('text', '')).strip()
                conf = float(r.get('confidence', 0.0))
                h = float(r.get('height', 0) or 0)
                w = float(r.get('width', 0) or 0)
                if len(txt) >= 3 and conf >= 0.6 and h > 5 and w > 5:
                    filtered.append(r)
            if len(filtered) < 2:
                logger.info("–ü–æ—Å–ª–µ –∂–µ—Å—Ç–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —à—É–º–æ–≤ –æ—Å—Ç–∞–ª–æ—Å—å < 2 —Ä–µ–≥–∏–æ–Ω–æ–≤ ‚Äî —Å—á–∏—Ç–∞–µ–º –æ–¥–∏–Ω —à—Ä–∏—Ñ—Ç")
                return False

            # 1. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
            heights = [region.get('height', 0) for region in filtered]
            heights = [h for h in heights if h > 5]  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ
            
            logger.info(f"–í—ã—Å–æ—Ç—ã –æ–±–ª–∞—Å—Ç–µ–π: {heights}")
            
            if len(heights) >= 2:
                import numpy as np
                heights_array = np.array(heights)
                # –†–æ–±–∞—Å—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –º–µ–¥–∏–∞–Ω–µ
                median_h = float(np.median(heights_array))
                mad = float(np.median(np.abs(heights_array - median_h)) + 1e-6)
                std_height = 1.4826 * mad
                mean_height = float(np.mean(heights_array))
                max_height = np.max(heights_array)
                min_height = np.min(heights_array)
                
                # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
                height_variation = std_height / median_h if median_h > 0 else 0
                # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
                height_ratio = max_height / min_height if min_height > 0 else 1
                
                logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã—Å–æ—Ç: —Å—Ä–µ–¥–Ω–µ–µ={mean_height:.1f}, –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ={std_height:.1f}")
                logger.info(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏: {height_variation:.3f}, —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {height_ratio:.2f}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤ (—á—É—Ç—å –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ)
                # 1. –ë–æ–ª—å—à–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–µ–¥–∏–∞–Ω—ã
                if height_variation > 0.8:
                    logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–æ–ª—å—à–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤")
                    return True
                
                # 2. –ë–æ–ª—å—à–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ vs –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç)
                if height_ratio > 3.0:
                    logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –±–æ–ª—å—à–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ (–∑–∞–≥–æ–ª–æ–≤–æ–∫/–æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç)")
                    return True
            
            # 2. –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—â–∞–¥–µ–π –æ–±–ª–∞—Å—Ç–µ–π
            areas = [region.get('area', 0) for region in filtered]
            areas = [a for a in areas if a > 25]  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ
            
            if len(areas) >= 2:
                import numpy as np
                areas_array = np.array(areas)
                area_ratio = np.max(areas_array) / np.min(areas_array) if np.min(areas_array) > 0 else 1
                
                logger.info(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π: {area_ratio:.2f}")
                
                if area_ratio > 3.5:
                    logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –±–æ–ª—å—à–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π")
                    return True
            
            # 3. –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            words = text_content.split()
            if len(words) >= 6:
                # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª–µ–π
                has_uppercase = any(word.isupper() and len(word) > 1 for word in words)
                has_lowercase = any(word.islower() and len(word) > 1 for word in words)
                has_mixed_case = any(word[0].isupper() and any(c.islower() for c in word[1:]) for word in words if len(word) > 1)
                has_numbers = any(any(c.isdigit() for c in word) for word in words)
                
                style_count = sum([has_uppercase, has_lowercase, has_mixed_case, has_numbers])
                
                logger.info(f"–ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª–µ–π: uppercase={has_uppercase}, lowercase={has_lowercase}, mixed={has_mixed_case}, numbers={has_numbers}")
                logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∏–ª–µ–π: {style_count}")
                
                # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∏–ª–µ–π + –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±–ª–∞—Å—Ç–µ–π
                if style_count >= 3 and len(filtered) >= 8:
                    logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—Ç–∏–ª–µ–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏")
                    return True
            
            # 4. –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤
            if len(heights) >= 4:
                clusters = self._cluster_heights(heights)
                logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–≤: {clusters}")
                
                if len(clusters) >= 2:
                    # –¢—Ä–µ–±—É–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –æ–±–æ–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ —è–≤–Ω—É—é —Ä–∞–∑–Ω–∏—Ü—É
                    cluster_means = [np.mean(cluster) for cluster in clusters]
                    cluster_sizes = [len(cluster) for cluster in clusters]
                    cluster_ratio = max(cluster_means) / min(cluster_means) if min(cluster_means) > 0 else 1
                    if cluster_ratio > 2.0 and min(cluster_sizes) >= 3:
                        logger.info("‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã 2+ —É—Å—Ç–æ–π—á–∏–≤—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞ —Ä–∞–∑–º–µ—Ä–æ–≤")
                        return True
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ–±–ª–∞—Å—Ç–µ–π (—Å—Ç—Ä–æ–≥–∞—è)
            if len(filtered) >= 12:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
                if len(heights) >= 3 and height_variation > 0.45:
                    logger.info("‚úÖ –ú–Ω–æ–≥–æ –æ–±–ª–∞—Å—Ç–µ–π —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –≤–∞—Ä–∏–∞—Ü–∏–µ–π —Ä–∞–∑–º–µ—Ä–æ–≤")
                    return True
            
            logger.info("‚ùå –ö—Ä–∏—Ç–µ—Ä–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã")
            return False
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏: {str(e)}")
            return False
    
    def _cluster_heights(self, heights: list, threshold: float = 0.3) -> list:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≤—ã—Å–æ—Ç –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø —Ä–∞–∑–º–µ—Ä–æ–≤"""
        if len(heights) < 2:
            return [heights]
        
        import numpy as np
        sorted_heights = sorted(heights)
        clusters = []
        current_cluster = [sorted_heights[0]]
        
        for height in sorted_heights[1:]:
            # –ï—Å–ª–∏ –≤—ã—Å–æ—Ç–∞ –±–ª–∏–∑–∫–∞ –∫ —Å—Ä–µ–¥–Ω–µ–º—É —Ç–µ–∫—É—â–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_mean = np.mean(current_cluster)
            relative_diff = abs(height - cluster_mean) / cluster_mean
            
            if relative_diff <= threshold:
                current_cluster.append(height)
            else:
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                clusters.append(current_cluster)
                current_cluster = [height]
        
        clusters.append(current_cluster)
        return clusters
    
    def _analyze_text_sizes_from_ocr(self, ocr_boxes: list) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ç–µ–∫—Å—Ç–∞ –∏–∑ OCR boxes –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤"""
        try:
            if not ocr_boxes or len(ocr_boxes) < 2:
                return {'multiple_fonts_detected': False, 'height_ratio': 1.0, 'area_ratio': 1.0}
            
            heights = []
            widths = []
            areas = []
            
            for box_info in ocr_boxes:
                if isinstance(box_info, dict) and 'bbox' in box_info:
                    bbox = box_info['bbox']
                    if isinstance(bbox, list) and len(bbox) >= 4:
                        if isinstance(bbox[0], list):  # –§–æ—Ä–º–∞—Ç [[x1,y1], [x2,y2], ...]
                            x_coords = [point[0] for point in bbox]
                            y_coords = [point[1] for point in bbox]
                        else:  # –§–æ—Ä–º–∞—Ç [x1, y1, x2, y2]
                            x_coords = [bbox[0], bbox[2]]
                            y_coords = [bbox[1], bbox[3]]
                        
                        width = max(x_coords) - min(x_coords)
                        height = max(y_coords) - min(y_coords)
                        
                        if height > 5 and width > 5:  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ
                            heights.append(height)
                            widths.append(width)
                            areas.append(width * height)
            
            if len(heights) < 2:
                return {'multiple_fonts_detected': False, 'height_ratio': 1.0, 'area_ratio': 1.0}
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è
            height_ratio = max(heights) / min(heights)
            area_ratio = max(areas) / min(areas)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
            height_std = np.std(heights)
            height_mean = np.mean(heights)
            height_cv = height_std / height_mean if height_mean > 0 else 0  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤ –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º
            multiple_fonts_detected = False
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–∏:
            # 1. –ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –≤—ã—Å–æ—Ç–µ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ vs –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç)
            if height_ratio > 2.5:
                multiple_fonts_detected = True
                logger.info(f"üìè –ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –≤—ã—Å–æ—Ç–µ: {height_ratio:.1f}")
            
            # 2. –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤
            if height_cv > 0.4:  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ > 40%
                multiple_fonts_detected = True
                logger.info(f"üìè –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤: {height_cv:.2f}")
            
            # 3. –ù–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä—É–ø–ø —Ä–∞–∑–º–µ—Ä–æ–≤
            if len(heights) >= 6:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
                height_groups = self._cluster_sizes(heights)
                if len(height_groups) >= 3:  # 3+ –≥—Ä—É–ø–ø—ã —Ä–∞–∑–º–µ—Ä–æ–≤
                    multiple_fonts_detected = True
                    logger.info(f"üìè –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(height_groups)} –≥—Ä—É–ø–ø —Ä–∞–∑–º–µ—Ä–æ–≤")
            
            return {
                'multiple_fonts_detected': multiple_fonts_detected,
                'height_ratio': height_ratio,
                'area_ratio': area_ratio,
                'height_cv': height_cv,
                'height_groups': len(self._cluster_sizes(heights)) if len(heights) >= 6 else 1
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑–º–µ—Ä–æ–≤: {str(e)}")
            return {'multiple_fonts_detected': False, 'height_ratio': 1.0, 'area_ratio': 1.0}
    
    def _cluster_sizes(self, sizes: list, threshold: float = 0.3) -> list:
        """–ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
        if len(sizes) < 2:
            return [sizes]
        
        sorted_sizes = sorted(sizes)
        clusters = []
        current_cluster = [sorted_sizes[0]]
        
        for size in sorted_sizes[1:]:
            # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –±–ª–∏–∑–æ–∫ –∫ —Ç–µ–∫—É—â–µ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–µ–≥–æ
            if abs(size - np.mean(current_cluster)) / np.mean(current_cluster) <= threshold:
                current_cluster.append(size)
            else:
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                clusters.append(current_cluster)
                current_cluster = [size]
        
        clusters.append(current_cluster)
        return clusters
    
    def _analyze_content_for_multiple_fonts(self, text_content: str, words: list, 
                                          has_uppercase: bool, has_lowercase: bool, 
                                          has_mixed_case: bool, has_all_caps: bool) -> dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤"""
        try:
            multiple_fonts_detected = False
            reasons = []
            
            # 1. –ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª–µ–π —Ç–µ–∫—Å—Ç–∞
            if has_uppercase and has_lowercase and has_mixed_case:
                # –°–º–µ—à–∞–Ω–Ω—ã–µ —Å—Ç–∏–ª–∏ –º–æ–≥—É—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ä–∞–∑–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã
                if len(words) > 5:  # –¢–æ–ª—å–∫–æ –¥–ª—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
                    multiple_fonts_detected = True
                    reasons.append("—Å–º–µ—à–∞–Ω–Ω—ã–µ —Å—Ç–∏–ª–∏ —Ç–µ–∫—Å—Ç–∞")
            
            # 2. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç)
            if len(words) >= 8:
                # –ò—â–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (–∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª–µ)
                first_words = words[:3]
                if any(len(word) <= 4 and word.isupper() for word in first_words):
                    if any(len(word) > 4 and not word.isupper() for word in words[3:6]):
                        multiple_fonts_detected = True
                        reasons.append("–∑–∞–≥–æ–ª–æ–≤–æ–∫ + –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç")
            
            # 3. –ê–Ω–∞–ª–∏–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            if has_numbers and len(words) > 3:
                # –¶–∏—Ñ—Ä—ã —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥—Ä—É–≥–æ–π —à—Ä–∏—Ñ—Ç
                number_words = [word for word in words if any(char.isdigit() for char in word)]
                text_words = [word for word in words if not any(char.isdigit() for char in word)]
                
                if len(number_words) >= 2 and len(text_words) >= 3:
                    multiple_fonts_detected = True
                    reasons.append("—Ü–∏—Ñ—Ä—ã + —Ç–µ–∫—Å—Ç")
            
            # 4. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ (–∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ–±—ã—á–Ω–æ –∫–æ—Ä–æ—á–µ)
            if len(words) >= 6:
                short_words = [word for word in words if len(word) <= 3]
                long_words = [word for word in words if len(word) >= 6]
                
                if len(short_words) >= 2 and len(long_words) >= 2:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∑–∏—Ü–∏–∏ - –µ—Å–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
                    short_positions = [i for i, word in enumerate(words) if len(word) <= 3]
                    if any(pos < 2 for pos in short_positions) or any(pos > len(words) - 3 for pos in short_positions):
                        multiple_fonts_detected = True
                        reasons.append("–∫–æ—Ä–æ—Ç–∫–∏–µ + –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏—è—Ö")
            
            return {
                'multiple_fonts_detected': multiple_fonts_detected,
                'reasons': reasons,
                'word_count': len(words),
                'has_mixed_styles': has_uppercase and has_lowercase and has_mixed_case,
                'has_numbers': any(char.isdigit() for char in text_content)
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {str(e)}")
            return {'multiple_fonts_detected': False, 'reasons': [], 'word_count': 0}
    
    def _calculate_multiple_fonts_score(self, regions_count: int, word_count: int, 
                                      size_analysis: dict, content_analysis: dict, 
                                      confidence: float) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤"""
        try:
            score = 0.0
            
            # 1. –†–∞–∑–º–µ—Ä—ã (–≤–µ—Å: 40%)
            if size_analysis['multiple_fonts_detected']:
                score += 0.4
            elif size_analysis['height_ratio'] > 2.0:
                score += 0.2
            elif size_analysis['height_cv'] > 0.3:
                score += 0.15
            
            # 2. –°–æ–¥–µ—Ä–∂–∏–º–æ–µ (–≤–µ—Å: 35%)
            if content_analysis['multiple_fonts_detected']:
                score += 0.35
            elif content_analysis['has_mixed_styles'] and word_count > 5:
                score += 0.2
            elif content_analysis['has_numbers'] and word_count > 3:
                score += 0.1
            
            # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (–≤–µ—Å: 15%)
            if regions_count > 15 and word_count > 10:
                score += 0.1
            elif regions_count > 20:
                score += 0.05
            
            # 4. –ö–∞—á–µ—Å—Ç–≤–æ OCR (–≤–µ—Å: 10%)
            if confidence > 0.8:
                score += 0.05  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR
            elif confidence < 0.5:
                score -= 0.05  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1]
            score = max(0.0, min(1.0, score))
            
            logger.info(f"üìä –û—Ü–µ–Ω–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤: {score:.3f}")
            logger.info(f"  - –†–∞–∑–º–µ—Ä—ã: {size_analysis.get('multiple_fonts_detected', False)}")
            logger.info(f"  - –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {content_analysis.get('multiple_fonts_detected', False)}")
            logger.info(f"  - –î–∞–Ω–Ω—ã–µ: {regions_count} —Ä–µ–≥–∏–æ–Ω–æ–≤, {word_count} —Å–ª–æ–≤")
            logger.info(f"  - OCR —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
            
            return score
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏: {str(e)}")
            return 0.0
    
    def _get_ocr_based_characteristics(self, ocr_result: dict) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¢–û–õ–¨–ö–û –∏–∑ OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            logger.info("=== –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö –ò–ó OCR ===")
            
            text_content = ocr_result.get('text_content', '').strip()
            regions_count = ocr_result.get('regions_count', 0)
            ocr_boxes = ocr_result.get('ocr_boxes', [])
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
            words = text_content.split()
            word_count = len(words)
            avg_word_length = np.mean([len(word) for word in words]) if words else 0
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏–∑ OCR boxes
            heights = []
            widths = []
            areas = []
            
            for box_info in ocr_boxes:
                if isinstance(box_info, list) and len(box_info) >= 2:
                    box = box_info[0]  # –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                    if isinstance(box, list) and len(box) >= 4:
                        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã box
                        x_coords = [point[0] for point in box]
                        y_coords = [point[1] for point in box]
                        
                        width = max(x_coords) - min(x_coords)
                        height = max(y_coords) - min(y_coords)
                        
                        heights.append(height)
                        widths.append(width)
                        areas.append(width * height)
            
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ OCR –¥–∞–Ω–Ω—ã—Ö
            characteristics = {
                'text_length': len(text_content),
                'word_count': word_count,
                'regions_count': regions_count,
                'avg_word_length': avg_word_length,
                'avg_height': np.mean(heights) if heights else 20.0,
                'avg_width': np.mean(widths) if widths else 100.0,
                'avg_area': np.mean(areas) if areas else 2000.0,
                'height_variance': np.var(heights) if len(heights) > 1 else 0.0,
                'width_variance': np.var(widths) if len(widths) > 1 else 0.0,
                'has_uppercase': any(c.isupper() for c in text_content),
                'has_lowercase': any(c.islower() for c in text_content),
                'has_numbers': any(c.isdigit() for c in text_content),
                'has_cyrillic': any(ord(c) >= 1040 and ord(c) <= 1103 for c in text_content),
                'text_density': word_count / max(regions_count, 1)  # —Å–ª–æ–≤ –Ω–∞ —Ä–µ–≥–∏–æ–Ω
            }
            
            logger.info(f"OCR —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {characteristics}")
            return characteristics
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è OCR —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {str(e)}")
            return self._get_default_ocr_characteristics()
    
    def _get_default_ocr_characteristics(self) -> dict:
        """OCR —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            'text_length': 0,
            'word_count': 0,
            'regions_count': 0,
            'avg_word_length': 0,
            'avg_height': 20.0,
            'avg_width': 100.0,
            'avg_area': 2000.0,
            'height_variance': 0.0,
            'width_variance': 0.0,
            'has_uppercase': False,
            'has_lowercase': False,
            'has_numbers': False,
            'has_cyrillic': False,
            'text_density': 0.0
        }
    
    def _binarize_image(self, gray: np.ndarray) -> np.ndarray:
        """–ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        binary = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
        return binary
    
    async def _extract_characteristics_from_ocr(self, image: np.ndarray, ocr_result: dict) -> FontCharacteristics:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —à—Ä–∏—Ñ—Ç–∞ –¢–û–õ–¨–ö–û –∏–∑ OCR"""
        
        logger.info("=== –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö –ß–ï–†–ï–ó OCR ===")
        
        # OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ –ø–æ–ª—É—á–µ–Ω –≤ –≤—ã–∑—ã–≤–∞—é—â–µ–º –º–µ—Ç–æ–¥–µ
        logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—Å—Ç–∞ (—Ä–∞–∑—Ä–µ—à–∞–µ–º 1 —Å–∏–º–≤–æ–ª)
        text_content = ocr_result.get('text_content', '').strip()
        if not text_content or len(text_content) < 1:
            logger.warning("‚ö†Ô∏è OCR –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç")
            raise ValueError("–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        confidence = ocr_result.get('confidence', 0.0)
        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –ø–æ—Ä–æ–≥ —Å –∫–æ–Ω—Ñ–∏–≥–æ–º –∫–∞—á–µ—Å—Ç–≤–∞; –ø—Ä–∏ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º, –Ω–æ –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        from ..config.ocr_config import get_text_quality_config
        quality_cfg = get_text_quality_config()
        min_avg = quality_cfg.get('min_avg_confidence', 0.05)
        if confidence < min_avg:
            logger.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å OCR: {confidence:.2f} < {min_avg:.2f}. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏.")
        
        # –ü–æ–ª—É—á–∞–µ–º OCR —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        ocr_chars = self._get_ocr_based_characteristics(ocr_result)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ FontCharacteristics –Ω–∞ –æ—Å–Ω–æ–≤–µ OCR –¥–∞–Ω–Ω—ã—Ö
        text_content = ocr_result.get('text_content', '').strip()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø —à—Ä–∏—Ñ—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –∏ —Ä–∞–∑–º–µ—Ä–∞–º
        has_serifs = self._predict_serifs_from_ocr(ocr_chars, text_content)
        
        # –†–ï–ê–õ–¨–ù–´–ï —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        # stroke_width –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ–ª—â–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
        if ocr_chars['avg_height'] > 0:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—â–∏–Ω—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
            stroke_width = min(1.0, max(0.0, ocr_chars['avg_height'] / 50.0))
        else:
            stroke_width = 0.5
        
        # contrast –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–π –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
        if ocr_chars['height_variance'] > 0 and ocr_chars['avg_height'] > 0:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            contrast = min(1.0, max(0.0, ocr_chars['height_variance'] / ocr_chars['avg_height']))
        else:
            contrast = 0.3
        
        # –ù–∞–∫–ª–æ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –Ω–∞–∫–ª–æ–Ω–∞
        slant = 0.0  # –ü–æ–∫–∞ –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–∫–ª–æ–Ω–∞
        
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        content_hash = hash(text_content + str(ocr_chars['regions_count']) + str(ocr_chars['avg_height']))
        unique_factor = (content_hash % 1000) / 1000.0
        
        # –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.info(f"üîç –£–ù–ò–ö–ê–õ–¨–ù–´–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø:")
        logger.info(f"  - –¢–µ–∫—Å—Ç: '{text_content[:50]}...' (–¥–ª–∏–Ω–∞: {len(text_content)})")
        logger.info(f"  - –†–µ–≥–∏–æ–Ω—ã: {ocr_chars['regions_count']}")
        logger.info(f"  - –°—Ä–µ–¥–Ω—è—è –≤—ã—Å–æ—Ç–∞: {ocr_chars['avg_height']:.2f}")
        logger.info(f"  - –•–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {content_hash}")
        logger.info(f"  - –£–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä: {unique_factor:.3f}")
        logger.info(f"  - stroke_width: {stroke_width:.3f}")
        logger.info(f"  - contrast: {contrast:.3f}")
        logger.info(f"  - slant: {slant:.3f}")
        
        # –ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ OCR
        avg_height = ocr_chars['avg_height']
        x_height = avg_height * 0.6  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –ø—Ä–æ–ø–æ—Ä—Ü–∏—è
        cap_height = avg_height
        ascender = avg_height * 1.2
        descender = avg_height * 0.3
        
        # –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
        letter_spacing = ocr_chars['avg_width'] / max(ocr_chars['avg_word_length'], 1) * 0.1
        word_spacing = ocr_chars['avg_width'] * 0.3
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0,1] –≤–æ –∏–∑–±–µ–∂–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        try:
            density_val = float(ocr_chars.get('text_density', 0.0))
        except Exception:
            density_val = 0.0
        density = max(0.0, min(1.0, density_val))
        
        # –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
        cyrillic_features = {
            'has_cyrillic': ocr_chars['has_cyrillic'],
            'cyrillic_ratio': 1.0 if ocr_chars['has_cyrillic'] else 0.0,
            'specific_letters': []
        }
        
        logger.info(f"OCR —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —à—Ä–∏—Ñ—Ç–∞: –∑–∞—Å–µ—á–∫–∏={has_serifs}, —Ç–æ–ª—â–∏–Ω–∞={stroke_width:.1f}, –≤—ã—Å–æ—Ç–∞={avg_height:.1f}")
        
        return FontCharacteristics(
            has_serifs=has_serifs,
            stroke_width=stroke_width,
            contrast=contrast,
            slant=slant,
            cyrillic_features=cyrillic_features,
            x_height=x_height,
            cap_height=cap_height,
            ascender=ascender,
            descender=descender,
            letter_spacing=letter_spacing,
            word_spacing=word_spacing,
            density=density
        )
    
    def _predict_serifs_from_ocr(self, ocr_chars: dict, text_content: str) -> bool:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è –∑–∞—Å–µ—á–µ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ OCR –¥–∞–Ω–Ω—ã—Ö"""
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π –∏ —Ä–∞–∑–º–µ—Ä—ã —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ - –≤–æ–∑–º–æ–∂–Ω–æ –∑–∞—Å–µ—á–∫–∏
        has_formal_text = any(word.lower() in ['–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π', '–¥–æ–∫—É–º–µ–Ω—Ç', '–∫–Ω–∏–≥–∞', '—Å—Ç–∞—Ç—å—è'] for word in text_content.split())
        stable_sizes = ocr_chars['height_variance'] < ocr_chars['avg_height'] * 0.2
        
        return has_formal_text and stable_sizes
    
    def _detect_serifs(self, binary: np.ndarray) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–ª–∏—á–∏—è –∑–∞—Å–µ—á–µ–∫"""
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ä–∞–∑–Ω–æ—Å—Ç—å - –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞—Å–µ—á–∫–∏)
        diff = cv2.absdiff(binary, opened)
        serif_pixels = np.sum(diff == 255)
        total_text_pixels = np.sum(binary == 0)  # –ß–µ—Ä–Ω—ã–µ –ø–∏–∫—Å–µ–ª–∏ –≤ –±–∏–Ω–∞—Ä–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        
        if total_text_pixels == 0:
            return False
        
        serif_ratio = serif_pixels / total_text_pixels
        has_serifs = serif_ratio > 0.05  # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞—Å–µ—á–µ–∫: —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ={serif_ratio:.3f}, —Ä–µ–∑—É–ª—å—Ç–∞—Ç={has_serifs}")
        return has_serifs
    
    def _analyze_stroke_width(self, binary: np.ndarray) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–ª—â–∏–Ω—ã —à—Ç—Ä–∏—Ö–æ–≤"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –Ω—É–ª—è (distance transform)
        dist_transform = cv2.distanceTransform(255 - binary, cv2.DIST_L2, 5)
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—Ä–µ–¥–Ω—é—é —Ç–æ–ª—â–∏–Ω—É —à—Ç—Ä–∏—Ö–æ–≤
        text_pixels = binary == 0
        if np.sum(text_pixels) == 0:
            return 0.1
        
        avg_thickness = np.mean(dist_transform[text_pixels]) * 2  # –£–º–Ω–æ–∂–∞–µ–º –Ω–∞ 2 –¥–ª—è –ø–æ–ª–Ω–æ–π —à–∏—Ä–∏–Ω—ã
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        normalized_thickness = avg_thickness / max(binary.shape)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [0, 1]
        normalized_thickness = normalized_thickness / 10.0  # –î–µ–ª–∏–º –Ω–∞ 10 –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        
        return min(1.0, max(0.0, normalized_thickness))
    
    def _analyze_contrast(self, gray: np.ndarray) -> float:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞"""
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∫–∞–∫ –º–µ—Ä—É –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
        std_dev = np.std(gray)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É 0-1
        contrast = std_dev / 128.0
        
        return min(1.0, max(0.0, contrast))
    
    def _analyze_slant(self, binary: np.ndarray) -> float:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞–∫–ª–æ–Ω–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –•–∞—Ñ–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏—Ö –ª–∏–Ω–∏–π
        edges = cv2.Canny(binary, 50, 150)
        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=50)
        
        if lines is None:
            return 0.0
        
        angles = []
        for line in lines[:20]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20 –ª–∏–Ω–∏–π
            rho, theta = line[0]
            angle = theta * 180 / np.pi
            
            # –ò–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ (–æ–∫–æ–ª–æ 90 –≥—Ä–∞–¥—É—Å–æ–≤)
            if 80 <= angle <= 100:
                angles.append(angle - 90)
        
        if not angles:
            return 0.0
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –Ω–∞–∫–ª–æ–Ω
        return np.mean(angles)
    
    def _analyze_geometry(self, binary: np.ndarray) -> Tuple[float, float, float, float]:
        """–ê–Ω–∞–ª–∏–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç—É—Ä—ã –±—É–∫–≤
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return 50.0, 70.0, 80.0, 20.0
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—ã—Å–æ—Ç—ã –±—É–∫–≤
        heights = []
        for contour in contours:
            _, _, _, h = cv2.boundingRect(contour)
            if h > 10:  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–æ–Ω—Ç—É—Ä—ã
                heights.append(h)
        
        if not heights:
            return 50.0, 70.0, 80.0, 20.0
        
        # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —à—Ä–∏—Ñ—Ç–æ–≤
        avg_height = np.mean(heights)
        x_height = avg_height * 0.5
        cap_height = avg_height * 0.7
        ascender = avg_height * 0.8
        descender = avg_height * 0.2
        
        return x_height, cap_height, ascender, descender
    
    def _analyze_spacing(self, binary: np.ndarray) -> Tuple[float, float]:
        """–ê–Ω–∞–ª–∏–∑ –º–µ–∂–±—É–∫–≤–µ–Ω–Ω—ã—Ö –∏ –º–µ–∂—Å–ª–æ–≤–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–µ–Ω –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
        height, width = binary.shape
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
        horizontal_projection = np.sum(binary == 0, axis=0)
        
        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏ –º–µ–∂–¥—É –±—É–∫–≤–∞–º–∏
        gaps = []
        in_gap = False
        gap_start = 0
        
        for i, projection in enumerate(horizontal_projection):
            if projection == 0 and not in_gap:  # –ù–∞—á–∞–ª–æ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∞
                in_gap = True
                gap_start = i
            elif projection > 0 and in_gap:  # –ö–æ–Ω–µ—Ü –ø—Ä–æ–º–µ–∂—É—Ç–∫–∞
                gaps.append(i - gap_start)
                in_gap = False
        
        if gaps:
            letter_spacing = np.mean(gaps)
            word_spacing = np.percentile(gaps, 75) if len(gaps) > 1 else letter_spacing * 2
        else:
            letter_spacing = 2.0
            word_spacing = 6.0
        
        return letter_spacing, word_spacing
    
    def _calculate_density(self, binary: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"""
        text_pixels = np.sum(binary == 0)
        total_pixels = binary.shape[0] * binary.shape[1]
        
        return text_pixels / total_pixels if total_pixels > 0 else 0.0
    
    def _analyze_cyrillic_features(self, binary: np.ndarray) -> CyrillicFeatures:
        """
        –ê–Ω–∞–ª–∏–∑ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö –±—É–∫–≤
        –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –±—É–∫–≤
        """
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        height, width = binary.shape
        text_density = self._calculate_density(binary)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–æ–π—Å—Ç–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        base_value = (height * width) % 100 / 100
        
        return CyrillicFeatures(
            ya_shape=0.5 + (text_density * base_value) % 0.4,
            zh_shape=0.6 + (width % 50) / 125,
            fi_shape=0.7 + (height % 30) / 100,
            shcha_shape=0.8 + (text_density * 100) % 20 / 100,
            yery_shape=0.5 + ((width + height) % 50) / 100
        )
    

    

    

    

    

    
# Fallback –º–µ—Ç–æ–¥—ã —É–¥–∞–ª–µ–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ PaddleOCR
    

    
# Fallback –º–µ—Ç–æ–¥—ã —É–¥–∞–ª–µ–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ PaddleOCR

